{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: RNN (Recurrent Neural Network) on TensorFlow\n",
    "\n",
    "In this project, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import TensorFlow and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: \n",
    "# operations return concrete values instead of constructing a computational graph to run later.\n",
    "tf.enable_eager_execution()\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load and Inspect the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Amir/.keras/datasets/shakespeare.txt\n",
      "The length of text is 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "There are 65 unique charachters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowenload the Shakespeare dataset to the cache_dir ~/.keras\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)\n",
    "\n",
    "# read the dataset in Binary mode (rb)\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(\"The length of text is {} characters\".format(len(text)))\n",
    "# see the first 200 charachters \n",
    "print(text[:200])\n",
    "\n",
    "# understand the unique characters in the text\n",
    "# get unique charachters in the text using set() and sort them in a list\n",
    "ch =sorted(set(text))\n",
    "print(\"There are {} unique charachters\".format(len(ch)))\n",
    "# type(ch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Vectorize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "'First Citizen' mapped to -> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text by mapping strings to a numerical representation\n",
    "\n",
    "# using enumerate loop over ch, retrieve both the index and the value of each item, and make a dictionary of it\n",
    "ch2idx = {u:i for i, u in enumerate(ch)}\n",
    "print(ch2idx)\n",
    "\n",
    "# transfrom text to integers\n",
    "text_as_int = np.array([ch2idx[c] for c in text])\n",
    "# inspect \n",
    "print ('{} mapped to -> {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "11153\n",
      "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
      " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
      " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
      " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
      " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
      " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ']\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# use tf to split data into manageable sequences \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# make an array of the unique charachters \n",
    "idx2ch = np.array(ch)\n",
    "\n",
    "# print examples of char_dataset; use idx2ch to find equivalent characters of the integers in char_dataset\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2ch[i])\n",
    "\n",
    "# the maximum length sentence (chunks) we want for a single input in characters\n",
    "seq_length = 100\n",
    "# tip: //: divide with integral result (discard remainder)\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "print(examples_per_epoch)\n",
    "\n",
    "# use batch method to convert these individual characters to sequences/chunks of of the desired size (100)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# print examples of sequences to get an impression \n",
    "for i in sequences.take(1):\n",
    "  print(idx2ch[i.numpy()])\n",
    "  print(repr(''.join(idx2ch[i.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "'re all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
     ]
    }
   ],
   "source": [
    "# define a function to shift our sequences/chunks to create target_text. For example, say seq_length is 4 and our text is \"Hello\". \n",
    "# The input sequence would be \"Hell\", and the target sequence \"ello\"\n",
    "def split_input_target(piece):\n",
    "    input_text = piece[:-1]\n",
    "    target_text = piece[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# we map our 100 long sequences to the function ro create shifted sequences \n",
    "sequences_s = sequences.map(split_input_target)\n",
    "\n",
    "# print an example of output to get an impression \n",
    "for input_example, target_example in sequences_s.take(2):\n",
    "    print(repr(''.join(idx2ch[input_example.numpy()])))\n",
    "    print(repr(''.join(idx2ch[target_example.numpy()]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    }
   ],
   "source": [
    "# shuffle the data and pack it into training batches\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "steps_per_epoch = examples_per_epoch//batch_size\n",
    "print(steps_per_epoch)\n",
    "\n",
    "# TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. \n",
    "# instead,it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# batch() will make bathes of batch_size from sequences_s dataset\n",
    "# shuffle() will allocate a buffer of size of batch_size for picking random entries from sequences_s\n",
    "sequences_s = sequences_s.shuffle(batch_size).batch(batch_size , drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build the RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
