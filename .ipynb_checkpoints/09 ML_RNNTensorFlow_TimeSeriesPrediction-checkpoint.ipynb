{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: RNN (Recurrent Neural Network) on TensorFlow\n",
    "\n",
    "In this project, ...\n",
    "This material is a re-write/repeat of material found in https://www.tensorflow.org/tutorials/sequences/text_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import TensorFlow and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow as tf\n",
    "\n",
    "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, without building graphs: \n",
    "# operations return concrete values instead of constructing a computational graph to run later.\n",
    "tf.enable_eager_execution()\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Load and Inspect the DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Amir/.keras/datasets/shakespeare.txt\n",
      "The length of text is 1115394 characters\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "There are 65 unique charachters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dowenload the Shakespeare dataset to the cache_dir ~/.keras\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt','https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "print(path_to_file)\n",
    "\n",
    "# read the dataset in Binary mode (rb)\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(\"The length of text is {} characters\".format(len(text)))\n",
    "# see the first 200 charachters \n",
    "print(text[:200])\n",
    "\n",
    "# understand the unique characters in the text\n",
    "# get unique charachters in the text using set() and sort them in a list\n",
    "ch =sorted(set(text))\n",
    "print(\"There are {} unique charachters\".format(len(ch)))\n",
    "# type(ch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Vectorize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "'First Citizen' mapped to -> [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text by mapping strings to a numerical representation\n",
    "\n",
    "# using enumerate loop over ch, retrieve both the index and the value of each item, and make a dictionary of it\n",
    "ch2idx = {u:i for i, u in enumerate(ch)}\n",
    "print(ch2idx)\n",
    "\n",
    "# transfrom text to integers\n",
    "text_as_int = np.array([ch2idx[c] for c in text])\n",
    "# inspect \n",
    "print ('{} mapped to -> {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Create Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n",
      "11153\n",
      "['F' 'i' 'r' 's' 't' ' ' 'C' 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'B' 'e' 'f'\n",
      " 'o' 'r' 'e' ' ' 'w' 'e' ' ' 'p' 'r' 'o' 'c' 'e' 'e' 'd' ' ' 'a' 'n' 'y'\n",
      " ' ' 'f' 'u' 'r' 't' 'h' 'e' 'r' ',' ' ' 'h' 'e' 'a' 'r' ' ' 'm' 'e' ' '\n",
      " 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'A' 'l' 'l' ':' '\\n' 'S' 'p' 'e' 'a'\n",
      " 'k' ',' ' ' 's' 'p' 'e' 'a' 'k' '.' '\\n' '\\n' 'F' 'i' 'r' 's' 't' ' ' 'C'\n",
      " 'i' 't' 'i' 'z' 'e' 'n' ':' '\\n' 'Y' 'o' 'u' ' ']\n",
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "# use tf to split data into manageable sequences \n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "# make an array of the unique charachters \n",
    "idx2ch = np.array(ch)\n",
    "\n",
    "# print examples of char_dataset; use idx2ch to find equivalent characters of the integers in char_dataset\n",
    "for i in char_dataset.take(10):\n",
    "    print(idx2ch[i])\n",
    "\n",
    "# the maximum length sentence (chunks) we want for a single input in characters\n",
    "seq_length = 100\n",
    "# tip: //: divide with integral result (discard remainder)\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "print(examples_per_epoch)\n",
    "\n",
    "# use batch method to convert these individual characters to sequences/chunks of of the desired size (100)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "# print examples of sequences to get an impression \n",
    "for i in sequences.take(1):\n",
    "  print(idx2ch[i.numpy()])\n",
    "  print(repr(''.join(idx2ch[i.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
      "'re all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "\n",
      "Each index of these vectors are processed as one time step. For the input at time step 0, the model receives the index for F and trys to predict the index for i as the next character. At the next timestep, it does the same thing but the RNN considers the previous step context in addition to the current input character.\n"
     ]
    }
   ],
   "source": [
    "# define a function to shift our sequences/chunks to create target_text. For example, say seq_length is 4 and our text is \"Hello\". \n",
    "# The input sequence would be \"Hell\", and the target sequence \"ello\"\n",
    "def split_input_target(piece):\n",
    "    input_text = piece[:-1]\n",
    "    target_text = piece[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "# we map our 100 long sequences to the function ro create shifted sequences \n",
    "sequences_s = sequences.map(split_input_target)\n",
    "\n",
    "# decode and print an example of output to get an impression \n",
    "for input_example, target_example in sequences_s.take(2):\n",
    "    print(repr(''.join(idx2ch[input_example.numpy()])))\n",
    "    print(repr(''.join(idx2ch[target_example.numpy()]))) \n",
    "\n",
    "\n",
    "print('')\n",
    "print(\n",
    "    \"Each index of these vectors are processed as one time step. \\\n",
    "For the input at time step 0, the model receives the index for F and \\\n",
    "trys to predict the index for i as the next character. At the next timestep, \\\n",
    "it does the same thing but the RNN considers the previous step context in addition to \\\n",
    "the current input character.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shuffle the data and pack it into training batches\n",
    "\n",
    "# Batch size\n",
    "batch_size = 64\n",
    "steps_per_epoch = examples_per_epoch//batch_size\n",
    "print(steps_per_epoch)\n",
    "\n",
    "# TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. \n",
    "# instead,it maintains a buffer in which it shuffles elements)\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "# batch() will make bathes of batch_size from sequences_s dataset\n",
    "# shuffle() will allocate a buffer of size of batch_size for picking random entries from sequences_s\n",
    "sequences_s = sequences_s.shuffle(batch_size).batch(batch_size , drop_remainder=True)\n",
    "sequences_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Build the RNN Model\n",
    "Use tf.keras.Sequential to define the model. For this  example three layers are used to define our model:\n",
    "\n",
    "- tf.keras.layers.Embedding: The input layer. A trainable lookup table that will map the numbers of each character to a vector with embedding_dim dimensions;\n",
    "- tf.keras.layers.GRU: A type of RNN with size units=rnn_units \n",
    "- tf.keras.layers.Dense: The output layer, with vocab_size outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model parameters \n",
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(ch)\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CuDNNGRU if running on GPU.\n",
    "if tf.test.is_gpu_available():\n",
    "  rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "  import functools\n",
    "  rnn = functools.partial(\n",
    "    tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RNN model\n",
    "\n",
    "# Keras is a high-level API to build and train deep learning models\n",
    "# tf.keras.Sequential: https://www.tensorflow.org/api_docs/python/tf/keras/models/Sequential#class_sequentialdef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "# Word Embedding is collective term for models that learned to map a set of words or phrases in a vocabulary to vectors of numerical values.\n",
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, embedding_dim,batch_input_shape=[batch_size, None]),\n",
    "                                 rnn(rnn_units,return_sequences=True,recurrent_initializer='glorot_uniform',stateful=True),\n",
    "                                 tf.keras.layers.Dense(vocab_size)])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image                                                                                \n",
    "# img = Image.open('text_generation_training.png')\n",
    "# print(\"# For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-liklihood of the next character:\")\n",
    "# img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (64, None, 256)           16640     \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (64, None, 65)            66625     \n",
      "=================================================================\n",
      "Total params: 4,018,497\n",
      "Trainable params: 4,018,497\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Try the \"untrained\" model\n",
    "\n",
    "# check the shape of the output\n",
    "for input_example, target_example in sequences_s.take(1):\n",
    "    target_example_prediction = model(input_example)\n",
    "    print(target_example_prediction.shape,\"# (batch_size, sequence_length, vocab_size)\")\n",
    "\n",
    "# try it for the first example in the batch    \n",
    "sampled_indices = tf.random.categorical(target_example_prediction[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
